%=================================================================
\section{Introduction}\label{sec-intro}

Data is the new oil for the industry and
it can be used for empowering processes with
new business-related insights
(e.g. to define new customer groups through predictive analytics).
At the same time data is a critical asset of a company and
can be used for detecting the anomaly behavior or event
such as for
intrusion detection,
fraud detection,
fault detection,
event detection in sensor networks,
and detecting eco-system disturbances.

An anomalier,
also refered as outlier, 
is defined by Hawkins~\cite{hawkins1980identification}
as follows:
``An outlier is an observation which
deviates so much from the other observations
as to arouse suspicions that
it was generated by a different mechanism.''
Based on this definiton,
two general approaches exist for the anomaly
detection in the industry:
rule-based method,
where we manually define some rules of
well-known anomaly behavior
with prior knowledge,
and data-based anomaly detection,
where looking for behavior that
is out of the normal from data.

The Rule-based method works reliably on known anomaly behavior,
but has the obvious disadvantage of not being capable of
detecting new anomaly behavior~\cite{}.
The data-based anomaly detection model works by
identifying anomalies by creating a model of
the normal patterns in the data,
and then compute an outlier score of a given data point
on the basis of the deviations from these patterns~\cite{chandola2009anomaly}.
The main advantage of data-based anomaly detection is that
it does not require prior knowledge of an intrusion and
thus can detect new anomaly behavior.

There are several metheds been proposed in  academia and 
used in industry.
Because there is no rigid definition of which
observation exactly is an anomalier,
every method relying on certain assumptions of
what qualifies as an anomalier.
Some popular models are based on 
the distribution of 
objects~\cite{siripanadorn2010anomaly,chandola2009anomaly,kromanis2013support},
the distance~\cite{knorr1997unified} 
between objects,
or on the density of
the neighborhood of 
an object~\cite{agyemang2004algorithm,breunig2000lof,papadimitriou2003loci},
or based on the ensemble method~\cite{zhou2012ensemble}.
These methods represent different attempts to make
the rather vague intuition about 
what anomalier are more concrete,
typically in an implicit,
procedural way.
However,
there still serverl challenges of anomaly detection exist
in the practical  process of industry:
\begin{enumerate}
    \item Few or no labeled data sets.
    \item Anomaly Detecion in Data stream.
    In the practical process of industry,
    the value of dataset is usually infinity.
    \item Explaining anomaly detection.
    To make  user convince and help them make decision,
    the proposed method could not only identify anomalies  but also
    why.
\end{enumerate}
This paper aims to review data-based anomaly detection methods and
identify unspervisd anomaly detection,
data stream anomaly, and explaining anomaly detection
to solve challenges above.

The rest of the chapter is organised as follows.
We first introduce the existing methods of classic anomaly detection,
in the Section~\ref{sec-method},
Then,
we introdeuce the explaining anomaly detect method
in the Section~\ref{sec-explain_anomaly}.
Section~\ref{sec:tools} introduce the published tools of
methods of anomaly detection,
followed by the conclusions in the last section.

\section{Method of Anomaly Detection}~\label{sec-method}

Many data-based anomaly detection methods have been proposed in
data mining literature.
The general methods can be divided into four categories as follows: 
statistical, 
distance, 
clustering and Ensemble~\cite{cook2019anomaly}.

\subsection{Statistical Based Methods}

Anomaly detection based on statistical is the earliest and
most studied approach.
It considers that the difference
between the data and the statistical distribution or the model
given is bigger than a particular value or range that is
anomaly~\cite{chandola2009anomaly}.
This method can be divided into two
categories: distribution-based method and depth-based
method~\cite{wu2016survey}.

The distribution-based method assigns a distribution (such as
normal distribution,
Poisson distribution,
etc.),
and then the anomaly is found by the method of consistency checking.
However,
the actual distribution of the data set is always unknown,
and it is difficult to
estimate the data distribution in high-dimensional;
To solve the problem, 
self-organizing map (SOM)~\cite{siripanadorn2010anomaly}, 
Support Vector Regression~\cite{kromanis2013support} (SVR) and
other machine learning based techniques are introduced to
improve those shortcomings.

The depth-based method considers that
each object is a point in n-dimensional space,
each point has one set depth,
and anomaly possibly exists in the object which has lower depth.
This process avoids the data distribution fitting problem of
distribution method of distribution based method.
But it needs to calculate
the convex closure of n-dimensional space and has higher
calculation complexity,
only suitable for low dimensional
data like two dimensional or three,
and has a low efficiency
on large data sets with four or higher dimensions~\cite{ruts1996computing}.

\subsection{Distance Based Methods}

The basic thought of anomaly detection based on
distance is to calculate the distance between data points in
data space by setting a distance function.
It is regarded as
abnormal when there is a large distance between a data
object and others.
The  firstly anomaly detection method 
based on distance is proposed in~\cite{knorr1997unified}.
It considers $o$ as $DB(p,d)$,
exception if the distance between object $o$ and
$p$ objects at least is greater than $d$.
And then,
the concept of this distance is extended to k-neighbor distance.
K-neighbor distance of each object is calculated and
sorted from small to big in~\cite{ramaswamy2000efficient},
the objects which have largest
distance are considered abnormal.

The anomaly detection based on distance combines the ideas
based on distribution,
overcomes the primary disadvantages
of anomaly detection based on distribution,
is easier to realize and comprehend,
and is widely studied and used.
But it has its own drawbacks.
Firstly,
the complexity of the algorithm is relatively high,
and it cannot take into account
the data size of data sets and the scalability of dimension.
Secondly,
it is pointed out distance based anomaly detection is flawed in
processing the data sets with obvious internal density
differences~\cite{breunig2000lof}.
It either considers the data in the area of sparse
density to be abnormal or
it cannot find some anomalous.
This is mainly because of the anomaly detection based on
distance considers all the viewpoints,
its capability is not good in processing the data which
contains a variety of distribution or
are mixed with different densities subset.
So,
its practical application is limited.

\subsection{Density Based Methods}
All the methods above have a common problem,
that is,
a global distance criterion is considered as
the basis for anomaly detection.
In fact,
the anomaly is usually detected
from the perspective of individual,
that is,
the anomaly point is far from its neighbor cluster.
So it is not appropriate
to use global distance.
To solve this problem,
several anomaly detection
algorithm based
on density is proposed .
Its basic idea is detecting anomaly by comparing
the density of object and its neighbor.
Breunig et al. introduce local
outlier factor (LOF),
considers the exception is not a
two-value property but a measure.
The LOF value higher,
the data is more likely to be abnormal~\cite{breunig2000lof}.
Agyemang et al.
proposed local sparsity coefficient (LCF) to
detect anomaly and reduce
the computational complexity by considering
the largest distance between the nearest k objects as
k-distance~\cite{agyemang2004algorithm}.
Furthermore,
Papadimitriou et al. got multi-granularity
deviation factor (MDEF) by
comparing the number of data objects in r-neighbor and
their mean values,
took it the measure of abnormal~\cite{papadimitriou2003loci}.
This method does not need to calculate the density of data
points,
and the computational efficiency is higher than LOF.
The idea based on density is closer to Hawkins’ exception
definition~\cite{hawkins1980identification}
than the idea based on distance.
So it can detect the local anomaly,
reduce the detection error which contains
a variety of distribution or are mixed with different
densities subset,
have a higher detection precision.
However,
its time complexity is still high,
the detection
results are sensitive to the selection of the parameters like
outlier factor threshold and the parameters are difficult to
determine.

\subsection{Ensembles Based Methods}

Because every model is specialized for different 
characteristics of observations and therefore fits 
only to some aspects of the “whole truth”,
it might be a good idea to integrate various different
outlier detection results,
producing a consensus of judgements.
The key idea of such an approach,
which is called an “ensemble”,
is that the combination of individual judgements,
or outlier detection results,
is beneficial if those judgements do not contain all the same errors.
One might think of it as a majority vote of a 
jury: 
One or another judgement about an observation might be wrong,
but the majority might still be right,
as long as the judgements are,
overall,
somewhat reliable and every member decides independently from the
others.

\section{Explain anomaly detection}
\label{sec-explain_anomaly}
Explaining anomaly detection is the task of making
the anomaly detection process more transparent to the users,
i.e. why are these samples are recorded as anomalies?
% The Explaining anomaly detection task refers to
% making users fully understand the anomaly behavior.
% However,
The  goal of explanation is not only to
convince the users about the proposed anomoaly detection method,
but also helping the users to understand behavior about
the .
This process will help the users to make decision such as
adjusting some feature of sample according to propsed method to
make the anomalier to normal.

In the last decade,
a few methods have been developed to explain anomaly.
% One approach uses an interpretable approximation of
% the original model~\cite{lundberg2017unified};
% examples of methods that
% use an interpretable approximation
% of the original model include: LIME~\cite{ribeiro2016should},
% which is an example of a
% model-agnostic method used to
% explain a prediction using a local model,
% and DeepLIFT~\cite{shrikumar2017learning},
% which is an example of a model-specific method for explaining
% deep learning models in which
% the contributions of all neurons in the network are
% back-propagated to the input features.
% SHAP - SHapley Additive exPlanation ~\cite{lundberg2017unified}
% combines previous methods for
% explaining predictions by calculating feature
% importance,
% using Shapley values from
% game theory that ensures consistency of
% the explanations.
Most of the existing works focus on mining anomalying features to 
produce the subspaces where 
the anomaliers exhibiting most abnormality. 
Dang et al.~\cite{dang2013local} introduced LODI to 
seek an optimal subspace where 
the distance between an anomaliers and 
inliers can be maximized. 
Micenkova et al.~\cite{micenkova2013explaining}
defined separability through the classification error of outliers and 
inliers, 
and leveraged it to 
quantify the explanation within every subspace. 
Then, 
they compared the measured explanations and 
selected the most explanatory subspace as the explanation of the outliers. 
Dang et al. ~\cite{dang2014discriminative} 
proposed LOGP to 
conduct a graph projection on outliers, 
which would further sort out the abnormal subspaces as the explanation. 
Duan et al. ~\cite{duan2015mining} 
demonstrated OAMiner, 
which focused on efficient mining of the discriminative subspaces. 
Macha et al.~\cite{macha2018explaining} 
demonstrated a pattern discover method providing explanations represented as hyper-ellipsoids. 
They conducted a clustering operation based on the density and 
purity of the involved instances before searching for the explanations. 
Then, 
several other literatures further derived rules to 
illustrate the deviation between anomaliers and 
inliers. 
Muller et al.~\cite{muller2012outrules} 
introduced OutRules to 
find normal and abnormal features and 
produce rules indicating the deviating behavior of anomaliers. 
He et al.~\cite{he2010co} 
demonstrated a method co-selecting the features and 
relevant anomaliers to 
describe the differences with inliers. 
% Clearly, 
% all of the above works [21]–[27] and 
% several implicit explanation works like [28]–[30] intended to 
% explain the anomaliers by providing a list of selected abnormal patterns, 
% which cannot be directly visualized and 
% are difficult to understand 
% by those unprofessional users.

% Unlike classic anomoaly detection method,
% debate and negotiation are often necessary for group users,
% and this calls for understanding of not only the pros but also
% the cons of the proposed recommendations.
% While existing explanation approaches focus on
% determining how good the recommendation is for the user,
% it is now desirable to know how bad the recommendation is for each group user.

\section{Evaluation of Anomaly Detection}

The evaluation metrics are essential for 
building successful anomaly detection system. 
Efforts have been made to identify the correct way of
measuring the quality of anomaly detection. 
This section
reviews common evaluation metrics from two aspects: 
supervised detection 
and unsupervised detection which is 
distinguished by whether have labeled dataset.

\subsection{Supervised Evaluation}
At present, 
the evaluation methods of supervised learning mainly
include confusion matrix method, accuracy and recall rate,
F1 score,
ROC curve,
AUC,
etc.

\subsubsection{Confusion Matrix}
For example,
for binary classification problems,
all problems are divided into 0 and 1.
The confusion matrix is shown in Table 1,
which is a 2 * 2 matrix.

\begin{table}  \centering
  \caption{Confusion Matrix}
  \label{tbl:overall-experiments}
  \begin{tabular}{ccc}
  \toprule
    & predicted value 0 & predicted value 1  \\
  \midrule
    True value 0 & TN & FP  \\
    True value 1 & FN & TP  \\
  \bottomrule
  \end{tabular}
\end{table}


TN:
the true value is 0,
and the predicted value is also 0,
that is,
our prediction is negative and the prediction is correct.

FP:
the true value is 0 and the predicted value is 1,
which means that our prediction is positive,
but the prediction is wrong.

FN:
the true value is 1 and the predicted value is 0,
 which means that our prediction is negative,
  but the prediction is wrong.

TP:
the true value is 1 and the predicted value is 1,
which means that our prediction is positive and the
prediction is correct.

\subsubsection{Precision}

The precision rate is to measure the proportion of
the number of correctly predicted samples to all
samples predicted as positive~\cite{ting2010precision}.

\begin{equation}
  precision=\frac{TP}{TP+FP}
\end{equation}

\subsubsection{Recall}
Recall rate refers to the number of correctly
predicted samples in all the data with true
value of 1.

\begin{equation}
  recall=\frac{TP}{TP+FN}
\end{equation}

\subsubsection{F1 Score}
F1 score is a harmonic average of accuracy and recall:
\begin{equation}
  F1=\frac{2*precision*recall}{precision+recall}
\end{equation}
The characteristic of harmonic average is that if
the two are extremely unbalanced,
for example, when one value is extremely
high and the other is extremely low,
the obtained F1 score value is also very low;
only when both of them are very high,
F1 will be high.
\subsubsection{ROC Curve}
TPR:
the recall rate is the percentage of the data
predicted to be 1 and the correct number in the
true value of 1.
\begin{equation}
TPR=recall
 \end{equation}

FPR:
the number of data with a prediction of 1,
but a wrong prediction,
as a percentage of the data whose true value is not 1.
\begin{equation}
  FPR=\frac{FP}{TN+FP}
\end{equation}
TPR is proportional to FPR,
and ROC curve is the curve describing these two relationships.
The closer the ROC curve is to the upper left corner,
the better the classifier is.

\subsubsection{AUC}
AUC (area under curve) refers to the area under the ROC curve.
The vertical and horizontal axis ranges are (0,1),
so the total area is less than 1~\cite{bradley1997use}.
The larger the AUC,
the better the classification effect.

Intuitively, 
the ROC AUC value can be seen as the probability that a pair
of two randomly chosen objects,
one positive example (outlier) 
and one negative example (inlier),
is sorted correctly
(i.e.,
the outlier is ranked before the inlier)~\cite{hanley1982meaning}.
ROC curves
and ROC AUC analysis inherently treat the class imbalance
problem by using the relative frequencies which makes them
particularly popular for evaluation of outlier detection

\subsection{Unsupervised Evaluation}
If some task have no labeled dataset,
the unsupervised evaluation method 
such as comparative evaluation,
generating pseudo tags,
and similarity analysis could be used.

\subsubsection{Comparative Evaluation}

For unsupervised learning, 
a common evaluation strategy is to rank the 
results according to the score of outliers, 
and then iteratively set the threshold from 
the first to the last. 
This will form n ancestor values 
(true positive rate and false positive rate), 
and a ROC curve can be obtained. 
The integral AUC of ROC can be used as a measure 
to test the performance.


\subsubsection{Generating Pseudo Tags}

There are a lot of learning efforts to transform 
unsupervised learning into supervised learning, 
and there are now feasible methods. 
Then, 
we can use the evaluation methods of supervised learning, 
such as accuracy.

\subsubsection{Similarity Analysis}

Unsupervised learning often depends 
on the similarity between data, 
which can be expressed as spatial density 
or distance measurement. 
In the evaluation of anomaly detection algorithm, 
it can be assumed that a large number of normal 
data are closely adjacent (can form multiple clusters), 
and outliers are often quite different from these normal points.

\section{Anomaly detection software packages}~\label{sec:tools}
Although many companies have implemented their own Anomaly detection systems
to accommodate their specific business needs,
there are still many free/ open source recommender system
software packages available.
In this section,
we review some popular software packages for practitioners
to build their anomaly detection systems.

\subsection{ADTK}

Anomaly Detection Toolkit (ADTK) (\href{https://adtk.readthedocs.io}{https://adtk.readthedocs.io}) is a Python package for
unsupervised / rule-based time series anomaly detection.

\subsection{PyOdds}

PyODDS (\href{http://pyodds.com/}{http://pyodds.com/}) is an end-to end Python system for
outlier detection with database support.
PyODDS provides outlier detection algorithms,
which support both static and time-series data.

\subsection{PyOD}

PyOD~\cite{zhao2019pyod} (\href{https://pypi.org/project/pyod/}{https://pypi.org/project/pyod/})
is a comprehensive and scalable Python toolkit for
detecting outlying objects in multivariate data.
It contains more than 20 detection algorithms,
including emerging deep learning models and
outlier ensembles.

\subsection{Anomaly Detection Toolbox}

Anomaly Detection Toolbox
(\href{http://dsmi-lab-ntust.github.io/AnomalyDetectionToolbox/}{http://dsmi-lab-ntust.github.io/AnomalyDetectionToolbox/})
collectes a lot of popular outlier detection algorithms in Matlab.

\subsection{DeepADoTS}

DeepADoTS (\href{https://github.com/KDD-OpenSource/DeepADoTS}{https://github.com/KDD-OpenSource/DeepADoTS})
is a benchmarking pipeline for anomaly detection on
time series data for
multiple state-of-the-art deep learning methods.

% \subsection{RRCF}
% RRCF~\cite{bartos_2019_rrcf} (\href{https://travis-ci.org/github/kLabUM/rrcf}{https://travis-ci.org/github/kLabUM/rrcf})

\section{Conclusions} \label{sec-conclusions}

Anomoaly detection has been extensively
attentted in recent years in the modern industry.
This chapter aimed to present the state of the art 
anomaly detecttion for practical using in industry.
This has included anomoaly detection method based on distribution, 
distance,
density,
cluster and ensemble.
To evaluate anomoaly detection method,
we reviewed commonly used 
two categories including supervised and unspervisd.
In addition,
we reviewd the explanation of anomoaly detection which
has been getting more and more attention. 
Finaly,
we provided a list of free and open source software packages for
practitioners to create their own anomaly detection systems.


There has been extensive research on the topic of anomoaly detection.
As this chapter provides only an introduction to this
topic,
we recommend a list of books and 
papers under Further Reading for readers.


% \blindtext

% % %\section*{Acknowledgment}

% % \lipsum[1]


% % The authors would like to thank \ldots

