%=================================================================
\section{Introduction}\label{sec-intro}

Data is the new oil for the industry and
it can be used for empowering processes with
new business-related insights
(e.g. to define new customer groups through predictive analytics).
At the same time data is a critical asset of a company and
can be used for detecting the anomaly behavior or event
such as for
intrusion detection~\cite{garcia2009anomaly},
fraud detection~\cite{bolton2002statistical},
fault detection~\cite{hwang2009survey} and
event detection in social networks~\cite{sakaki2010earthquake}.

An anomalier,
also refered as outlier, 
is defined by Hawkins~\cite{hawkins1980identification}
as follows: 
\begin{quotation}
``An outlier is an observation which
deviates so much from the other observations
as to arouse suspicions that
it was generated by a different mechanism.''
\end{quotation}
Based on this definiton,
two general approaches exist for the anomaly
detection in the industry:
rule-based method,
where we manually define some rules of
well-known anomaly behavior
with prior knowledge,
and data-based anomaly detection,
where looking for behavior that
is out of the normal from data.

The Rule-based method works reliably on known anomaly behavior,
but has the obvious disadvantage of not being capable of
detecting new anomaly behavior~\cite{}.
The data-based anomaly detection model works by
identifying anomalies by creating a model of
the normal patterns in the data,
and then compute an outlier score of a given data point
on the basis of the deviations from these patterns~\cite{chandola2009anomaly}.
The main advantage of data-based anomaly detection is that
it does not require prior knowledge of an intrusion and
thus can detect new anomaly behavior.

Many metheds have been proposed in academia and 
widely used in industry.
Because there is no rigid definition of which
observation exactly is an anomalier,
every method relying on certain assumptions of
what qualifies as an anomalier.
Some popular models are based on 
the distribution of 
objects~\cite{siripanadorn2010anomaly,chandola2009anomaly,kromanis2013support},
the distance~\cite{knorr1997unified} 
between objects,
or on the density of
the neighborhood of 
an object~\cite{agyemang2004algorithm,breunig2000lof,papadimitriou2003loci},
or based on the ensemble method~\cite{zhou2012ensemble}.
These methods represent different attempts to make
the rather vague intuition about 
what anomalier are more concrete,
typically in an implicit,
procedural way.
However,
serverl challenges still exist
in the practical  process of industry:
\begin{enumerate}
    \item There are few or no labeled data sets in 
    many real industry systems.
    % \item Anomaly Detecion in Data stream.
    % In the practical process of industry,
    % the value of dataset is usually infinity.
    \item Explaining anomaly detection.
    To make  user convince and help them make decision,
    the proposed method could not only identify anomalies but also
    why.
\end{enumerate}
This paper aims to review data-based anomaly detection methods and
identify unspervisd anomaly detection and explaining anomaly detection
to solve challenges above.

The rest of the chapter is organised as follows.
We first introduce the existing methods of classic anomaly detection,
in the Section~\ref{sec-method},
Then,
we introdeuce the explaining anomaly detect method
in the Section~\ref{sec-explain_anomaly}.
Section~\ref{sec:tools} introduce the published tools of
methods of anomaly detection,
followed by the conclusions in the last section.

\section{Method of Anomaly Detection}~\label{sec-method}

Many data-based anomaly detection methods have been proposed in
data mining literature.
The general methods can be divided into four categories as follows: 
statistical, 
distance, 
clustering and Ensemble~\cite{cook2019anomaly}.

\subsection{Statistical Based Methods}

Anomaly detection based on statistical is the earliest and
most studied method.
A data object is considered as anomalier when the differences
between the data and the statistical distribution or the model
given is bigger than a particular value or range ~\cite{chandola2009anomaly}.
This method can be divided into two
categories: distribution-based method and depth-based
method~\cite{wu2016survey}.

As for distribution-based method,
after given a distribution (such as
normal distribution,
Poisson distribution,
etc.), 
a method of consistency checking is used to find anomalier.
However,
the actual distribution of the data set is always unknown,
and it is another huge challenge to 
estimate the data distribution in high-dimensional. 
To solve those problem, 
self-organizing map (SOM)~\cite{siripanadorn2010anomaly}, 
Support Vector Regression~\cite{kromanis2013support} (SVR) and
other machine learning based techniques are introduced to
improve those shortcomings.

The depth-based method considers that
each object is a point in n-dimensional space
with one set depth,
and anomaly possibly exists in the object which has lower depth.
This process avoids the data distribution estimating problem of 
distribution based method.
However,
a convex closure of n-dimensional space needs to been calculate.
Therefore,
it has higher calculation complexity,
only suitable for low dimensional,
and has a low efficiency
on large data sets with higher dimensions~\cite{ruts1996computing}.
The principal component analysis (PCA)~\cite{deng2013modified}, 
independent component analysis (ICA) and 
partial least squares (PLS)~\cite{yin2014improved} is
widely used in this category.

\subsection{Distance Based Methods}

The basic idea of distance based anomaly detection
is to calculate the distance between data points in
data space after setting a distance function.
A data object is regarded as anomalier when 
the distance between  itself and others is large.
The firstly anomaly detection method 
based on distance is proposed in~\cite{knorr1997unified}.
Then it is extended to using K-neighbor distance to 
build the anomaly detector~\cite{ramaswamy2000efficient,kuang2008anomaly}.
The K-neighbor distance of each object is calculated and
sorted from small to big,
the objects which have largest
distance are considered as anomalies.

The distance based anomaly detection
is easier to realize,
and is widely studied and used.
But it has some disadvantages.
Firstly,
the complexity of the algorithm is relatively high,
such as the computation complexity of KNN is $O(m^2)$,
and it cannot consider the size of 
data sets and the scalability of data dimension.
Secondly,
it is pointed out distance based anomaly detection become invalid when 
the data sets has obvious internal density
differences~\cite{breunig2000lof}.
Its capability is not good in processing the data which
contains a variety of distribution or
are mixed with different densities subset.
This is mainly because of the anomaly detection based on
distance considers from all the viewpoints.
Thus the practical application  of distance based methods is limited.

\subsection{Density Based Methods}
All the methods above have a common problem,
that is,
a global distance criterion is considered as
the basis for anomaly detection.
In fact,
the anomaly is usually detected
from the perspective of individual,
that is,
the anomaly point is far from its neighbor cluster.
So it is not appropriate
to use global distance.
To solve this problem,
several anomaly detection
algorithm based
on density is proposed .
Its basic idea is detecting anomaly by comparing
the density of object and its neighbor.
Breunig et al. introduce local
outlier factor (LOF),
considers the exception is not a
two-value property but a measure.
The LOF value higher,
the data is more likely to be abnormal~\cite{breunig2000lof}.
Agyemang et al.
proposed local sparsity coefficient (LCF) to
detect anomaly and reduce
the computational complexity by considering
the largest distance between the nearest k objects as
k-distance~\cite{agyemang2004algorithm}.
Furthermore,
Papadimitriou et al. got multi-granularity
deviation factor (MDEF) by
comparing the number of data objects in r-neighbor and
their mean values,
took it the measure of abnormal~\cite{papadimitriou2003loci}.
This method does not need to calculate the density of data
points,
and the computational efficiency is higher than LOF.
The idea based on density is closer to Hawkins’ exception
definition~\cite{hawkins1980identification}
than the idea based on distance.
So it can detect the local anomaly,
reduce the detection error which contains
a variety of distribution or are mixed with different
densities subset,
have a higher detection precision.
However,
its time complexity is still high,
the detection
results are sensitive to the selection of the parameters like
outlier factor threshold and the parameters are difficult to
determine.

\subsection{Ensembles Based Methods}

Because every model is specialized for different 
characteristics of observations and therefore fits 
only to some aspects of the “whole truth”,
it might be a good idea to integrate various different
outlier detection results,
producing a consensus of judgements.
The key idea of such an approach,
which is called an “ensemble”,
is that the combination of individual judgements,
or outlier detection results,
is beneficial if those judgements do not contain all the same errors.
One might think of it as a majority vote of a 
jury: 
One or another judgement about an observation might be wrong,
but the majority might still be right,
as long as the judgements are,
overall,
somewhat reliable and every member decides independently from the
others.

\section{Explain anomaly detection}
\label{sec-explain_anomaly}
Explaining anomaly detection is the task of making
the anomaly detection process more transparent to the users,
i.e. why are these samples are recorded as anomalies?
% The Explaining anomaly detection task refers to
% making users fully understand the anomaly behavior.
% However,
The  goal of explanation is not only to
convince the users about the proposed anomoaly detection method,
but also helping the users to understand behavior about
the .
This process will help the users to make decision such as
adjusting some feature of sample according to propsed method to
make the anomalier to normal.

In the last decade,
a few methods have been developed to explain anomaly.
% One approach uses an interpretable approximation of
% the original model~\cite{lundberg2017unified};
% examples of methods that
% use an interpretable approximation
% of the original model include: LIME~\cite{ribeiro2016should},
% which is an example of a
% model-agnostic method used to
% explain a prediction using a local model,
% and DeepLIFT~\cite{shrikumar2017learning},
% which is an example of a model-specific method for explaining
% deep learning models in which
% the contributions of all neurons in the network are
% back-propagated to the input features.
% SHAP - SHapley Additive exPlanation ~\cite{lundberg2017unified}
% combines previous methods for
% explaining predictions by calculating feature
% importance,
% using Shapley values from
% game theory that ensures consistency of
% the explanations.
Most of the existing works focus on mining anomalying features to 
produce the subspaces where 
the anomaliers exhibiting most abnormality. 
Dang et al.~\cite{dang2013local} introduced LODI to 
seek an optimal subspace where 
the distance between an anomaliers and 
inliers can be maximized. 
Micenkova et al.~\cite{micenkova2013explaining}
defined separability through the classification error of outliers and 
inliers, 
and leveraged it to 
quantify the explanation within every subspace. 
Then, 
they compared the measured explanations and 
selected the most explanatory subspace as the explanation of the outliers. 
Dang et al. ~\cite{dang2014discriminative} 
proposed LOGP to 
conduct a graph projection on outliers, 
which would further sort out the abnormal subspaces as the explanation. 
Duan et al. ~\cite{duan2015mining} 
demonstrated OAMiner, 
which focused on efficient mining of the discriminative subspaces. 
Macha et al.~\cite{macha2018explaining} 
demonstrated a pattern discover method providing explanations represented as hyper-ellipsoids. 
They conducted a clustering operation based on the density and 
purity of the involved instances before searching for the explanations. 
Then, 
several other literatures further derived rules to 
illustrate the deviation between anomaliers and 
inliers. 
Muller et al.~\cite{muller2012outrules} 
introduced OutRules to 
find normal and abnormal features and 
produce rules indicating the deviating behavior of anomaliers. 
He et al.~\cite{he2010co} 
demonstrated a method co-selecting the features and 
relevant anomaliers to 
describe the differences with inliers. 
% Clearly, 
% all of the above works [21]–[27] and 
% several implicit explanation works like [28]–[30] intended to 
% explain the anomaliers by providing a list of selected abnormal patterns, 
% which cannot be directly visualized and 
% are difficult to understand 
% by those unprofessional users.

% Unlike classic anomoaly detection method,
% debate and negotiation are often necessary for group users,
% and this calls for understanding of not only the pros but also
% the cons of the proposed recommendations.
% While existing explanation approaches focus on
% determining how good the recommendation is for the user,
% it is now desirable to know how bad the recommendation is for each group user.

\section{Evaluation of Anomaly Detection}

The evaluation metrics are essential for 
building successful anomaly detection system. 
Efforts have been made to identify the correct way of
measuring the quality of anomaly detection. 
This section
reviews common evaluation metrics from two aspects: 
supervised detection 
and unsupervised detection which is 
distinguished by whether have labeled dataset.

\subsection{Supervised Evaluation}
When the labeled dataset is valuable,
the supervised evaluation method could be used. 
At present, 
the popular evaluation methods of supervised learning
include accuracy and recall,
F1 score,
ROC curve,
and AUC.

\subsubsection{Confusion Matrix}
The confusion matrix,
as shown in Table~\ref{tb:confusion}, 
is a table two dimensions 
where each row of the matrix represents the instances 
in a predicted class while 
each column represents the instances 
in an actual class~\cite{powers2011evaluation}.
TP means true positives 
(i.e. items correctly labeled as belonging to the positive
class), 
FP means false positives 
(i.e. items incorrectly labeled as belonging to the
positive class), 
FN means false negatives 
(i.e. items which were not labeled as
belonging to the positive class but should have been).
TN means true negative 
(i.e. items correctly labeled as belonging to the negative
class).

\begin{table}  \centering
  \caption{Confusion Matrix. 
  TP is True Positive; 
  FP is False Positive; 
  TN is True Negative; 
  FN is False Negative. }
  \label{tb:confusion}
  \begin{tabular}{ccc}
  \toprule
    & Actual value   &  Actual value    \\
  \midrule
  Predicted value  & TP & FP  \\
  Predicted value    & FN & TN  \\
  \bottomrule
  \end{tabular}
\end{table}


\subsubsection{Precision}

The \textit{Precision} Precision
measures the ratio of examples classied as positive that 
are truly positive~\cite{ting2010precision}.

\begin{equation}
  precision=\frac{TP}{TP+FP}
\end{equation}

\subsubsection{Recall}
The \textit{Recall} measures the ratio of positive examples that
 are correctly labeled.

\begin{equation}
  recall=\frac{TP}{TP+FN}
\end{equation}

\subsubsection{F1 Score}
The \textit{F1 score} is a tradeoff between 
the \textit{Precision} and \textit{Recall}:
\begin{equation}
  F1=\frac{2*precision*recall}{precision+recall}
\end{equation}
F1 metric weights recall and
precision equally, 
and a good detection algorithm will 
maximize both precision and
recall simultaneously.
Moderately good performance on 
both will be favored over
extremely good performance on 
one and poor performance on the other.

\subsubsection{ROC Curve}


\subsubsection{AUC}
\textit{AUC} (area under curve) refers to the area under the \textit{ROC curve}.
The vertical and horizontal axis ranges are (0,1),
so the total area is less than 1~\cite{bradley1997use}.
The larger the \textit{AUC},
the better the classification effect.

The \textit{AUC} value can be seen as the probability that a pair
of two randomly chosen objects,
one positive example (outlier) 
and one negative example (inlier),
is sorted correctly
(i.e.,
the outlier is ranked before the inlier)~\cite{hanley1982meaning}.
\textit{ROC curves}
and \textit{AUC} analysis inherently treat the class imbalance
problem by using the relative frequencies which makes them
particularly popular for evaluation of outlier detection.


\subsection{Unsupervised Evaluation}
If some task have no labeled dataset,
the unsupervised evaluation method 
such as comparative evaluation,
generating pseudo tags,
and similarity analysis could be used.

\subsubsection{Comparative Evaluation}

For unsupervised learning, 
a common evaluation strategy is to rank the 
results according to the score of outliers, 
and then iteratively set the threshold from 
the first to the last. 
This will form n ancestor values 
(true positive rate and false positive rate), 
and a ROC curve can be obtained. 
The integral AUC of ROC can be used as a measure 
to test the performance.


\subsubsection{Generating Pseudo Tags}

There are a lot of learning efforts to transform 
unsupervised learning into supervised learning, 
and there are now feasible methods. 
Then, 
we can use the evaluation methods of supervised learning, 
such as accuracy.

\subsubsection{Similarity Analysis}

Unsupervised learning often depends 
on the similarity between data, 
which can be expressed as spatial density 
or distance measurement. 
In the evaluation of anomaly detection algorithm, 
it can be assumed that a large number of normal 
data are closely adjacent (can form multiple clusters), 
and outliers are often quite different from these normal points.

\section{Anomaly detection software packages}~\label{sec:tools}
Although many companies have implemented their own Anomaly detection systems
to accommodate their specific business needs,
there are still many free/ open source recommender system
software packages available.
In this section,
we review some popular software packages for practitioners
to build their anomaly detection systems.

\subsection{ADTK}

Anomaly Detection Toolkit (ADTK) (\href{https://adtk.readthedocs.io}{https://adtk.readthedocs.io}) is a Python package for
unsupervised / rule-based time series anomaly detection.

\subsection{PyOdds}

PyODDS (\href{http://pyodds.com/}{http://pyodds.com/}) is an end-to end Python system for
outlier detection with database support.
PyODDS provides outlier detection algorithms,
which support both static and time-series data.

\subsection{PyOD}

PyOD~\cite{zhao2019pyod} (\href{https://pypi.org/project/pyod/}{https://pypi.org/project/pyod/})
is a comprehensive and scalable Python toolkit for
detecting outlying objects in multivariate data.
It contains more than 20 detection algorithms,
including emerging deep learning models and
outlier ensembles.

\subsection{Anomaly Detection Toolbox}

Anomaly Detection Toolbox
(\href{http://dsmi-lab-ntust.github.io/AnomalyDetectionToolbox/}{http://dsmi-lab-ntust.github.io/AnomalyDetectionToolbox/})
collectes a lot of popular outlier detection algorithms in Matlab.

\subsection{DeepADoTS}

DeepADoTS (\href{https://github.com/KDD-OpenSource/DeepADoTS}{https://github.com/KDD-OpenSource/DeepADoTS})
is a benchmarking pipeline for anomaly detection on
time series data for
multiple state-of-the-art deep learning methods.

% \subsection{RRCF}
% RRCF~\cite{bartos_2019_rrcf} (\href{https://travis-ci.org/github/kLabUM/rrcf}{https://travis-ci.org/github/kLabUM/rrcf})

\section{Conclusions} \label{sec-conclusions}

Anomoaly detection has been extensively
attentted in recent years in the modern industry.
This chapter aimed to present the state of the art 
anomaly detecttion for practical using in industry.
This has included anomoaly detection method based on distribution, 
distance,
density,
cluster and ensemble.
To evaluate anomoaly detection method,
we reviewed commonly used 
two categories including supervised and unspervisd.
In addition,
we reviewd the explanation of anomoaly detection which
has been getting more and more attention. 
Finaly,
we provided a list of free and open source software packages for
practitioners to create their own anomaly detection systems.


There has been extensive research on the topic of anomoaly detection.
As this chapter provides only an introduction to this
topic,
we recommend a list of books and 
papers under Further Reading for readers.


% \blindtext

% % %\section*{Acknowledgment}

% % \lipsum[1]


% % The authors would like to thank \ldots

