%=================================================================
\section{Introduction}\label{sec-intro}

Data is the new oil for the industry and
it can be used for empowering processes with
new business-related insights
(e.g. to define new customer groups through predictive analytics).
At the same time data is a critical asset of a company and
can be used for detecting the anomaly behavior or event
such as for
intrusion detection,
fraud detection,
fault detection,
event detection in sensor networks,
and detecting eco-system disturbances.

Anomalies are also refered as outlier in the 
data mining literature.
Hawkins~\cite{hawkins1980identification} defined
an outlier as follows:
``An outlier is an observation which 
deviates so much from the other observations
as to arouse suspicions that 
it was generated by a different mechanism.''
Based on this definiton, 
two general approaches exist for the anomaly 
detection in the industry: 
rule-based method, 
where we manually define some rules of  
well-known anomaly behavior
with prior knowledge, 
and data-based anomaly detection, 
where looking for behavior that
is out of the normal. 

The Rule-based method works reliably on known anomaly behavior, 
but has the obvious disadvantage of not being capable of 
detecting new anomaly behavior. 
The data-based anomaly detection model works by 
identifying anomalies by creating a model of 
the normal patterns in the data,
and then compute an outlier score of a given data point 
on the basis of the deviations from these patterns. 
The main advantage of data-based anomaly detection is that
it does not require prior knowledge of an intrusion and
thus can detect new anomaly behavior.
In this chapter,
we mainly discuss the data-based anomaly detection methods .


The challenges of anomaly detection in the industry:
\begin{itemize}
    \item There are few labeled data sets.
    \item Data stream.
    \item Explaining anomaly detection.
\end{itemize}

The rest of the chapter is organised as follows.
We first introduce the anomaly detection and
discuss the challenges of it in industry in section.
Then,
in the Section~\ref{sec-method},
we introduce the existing methods of anomaly detection in
industry and show how they solve the challenges above.
Section~\ref{sec:tools} introduce the published tools of
methods of anomaly detection,
followed by the conclusions in the last section.

\section{Method of Anomaly Detection}~\label{sec-method}

\subsection{Statistical Based Methods}

Anomaly detection based on statistical is the earliest and
most studied approach.
It considers that the difference
between the data and the statistical distribution or the model
given is bigger than a particular value or range that is
anomaly .
This method can be divided into two
categories: distribution-based method and depth-based
method.
The former is to assign a distribution (such as
normal distribution,
Poisson distribution,
etc.),
and then the anomaly is found by the method of consistency checking.
The vast majority tests of anomaly detection based on
distribution are for a single attribute while a large number
of abnormal data mining in reality is carried out in a
multidimensional space.
In addition,
the actual distribution
of the data set is often unknown,
and it is difficult to
estimate the data distribution in high-dimensional; The
latter method considers that each object is a point in
n-dimensional space,
each point has one set depth,
and anomaly possibly exists in the object which has lower depth.
This process avoids the data distribution fitting problem of
distribution method of distribution based method and has
better efficiency on UTS detection.
But it needs to calculate
the convex closure of n-dimensional space and has higher
calculation complexity,
only suitable for low dimensional
data like two dimensional or three,
and has a low efficiency
on large data sets with four or higher dimensions.

\subsection{Distance Based Methods}

The basic thought of anomaly detection based on
distance is to calculate the distance between data points in
data space by setting a distance function.
It is regarded as
abnormal when there is a large distance between a data
object and others.
Knorr et al.
firstly proposed an anomaly
detection method based on distance .
They consider o
as,
exception if the distance between object o and
p objects at least is greater than d.
And then,
the concept of
this distance is extended to k-neighbor distance.
K-neighbor
distance of each object is calculated and
sorted from small to big by Ramaswamy et al,
the objects which have largest
distance are considered abnormal .
At present,
there are several anomaly detection methods based on distance such as
the algorithm based on index,
the algorithm based on
nested loop and the algorithm based on element.
The anomaly detection based on distance combines the ideas
based on distribution,
overcomes the primary disadvantages
of anomaly detection based on distribution,
is easier to
realize and comprehend,
and is widely studied and used.
But it has its own drawbacks.
Firstly,
the complexity of the
algorithm is relatively high,
and it cannot take into account
the data size of data sets and the scalability of dimension.
The time complexity of the algorithm based on index and
the algorithm based on nested loop can reach O (Mn2).
The
time complexity of the algorithm based on element is O
(cm+n),
in which m means the dimension,
n represents the
data object in data set,
C represents the number of units;
Secondly,
Breunig et al.
pointed out in Literature that
anomaly detection based on distance is flawed in
processing the data sets with obvious internal density
differences.
It either considers the data in the area of sparse
density to be abnormal or it cannot find some anomalous.
This is mainly because of the anomaly detection based on
distance considers all the viewpoints,
its capability is not
good in processing the data which contains a variety of
distribution or are mixed with different densities subset.
In addition,
anomaly detection based on distance is very
sensitive to parameters p and d,
which requires the users
having a certain expertise to set the reasonable parameters.
So,
its practical application is limited.


\subsection{Density Based Methods}
All the methods above have a common problem,
that is,
a global distance criterion is considered as 
the basis for anomaly detection.
In fact,
the anomaly is usually detected
from the perspective of individual,
that is,
the anomaly point is far from its neighbor cluster.
So it is not appropriate
to use global distance.
To solve this problem,
Breunig and other scholars proposed anomaly detection 
algorithm based
on density.
Its basic idea is detecting anomaly by comparing
the density of object and its neighbor.
It introduces local
outlier factor (LOF),
considers the exception is not a
two-value property but a measure.
The LOF value higher,
the data is more likely to be abnormal.
Agyemang et al.
proposed local sparsity coefficient (LCF) to detect anomaly
and reduce the computational complexity by considering
the largest distance between the nearest k objects as
k-distance.
Furthermore,
Papadimitriou and other scholars got multi-granularity 
deviation factor (MDEF) by
comparing the number of data objects in r-neighbor and
their mean values,
took it the measure of abnormal.
This method does not need to calculate the density of data
points,
and the computational efficiency is higher than LOF.
The idea based on density is closer to Hawkinsâ€™ exception
definition than the idea based on distance.
So it can detect the local anomaly,
reduce the detection error which contains
a variety of distribution or are mixed with different
densities subset,
have a higher detection precision.
However,
there are some problems in the method based on
density,
its time complexity is still high,
the detection
results are sensitive to the selection of the parameters like
outlier factor threshold and the parameters are difficult to
determine.
\subsection{Ensembles Based Methods}

Ensemble methods train multiple learners and 
then combine them for use~\cite{zhou2012ensemble}.  
They have already achieved great success in many 
real-world tasks.
\xhanMarker


\section{Anomaly detection for data stream}



\section{Explain anomaly detection}
Explaining anomaly detection is the task of making 
the anomaly detection process more transparent to the users,
i.e. why are these samples are recorded as anomalies?  
% The Explaining anomaly detection task refers to 
% making users fully understand the anomaly behavior.
% However,
The  goal of explanation is not only to 
convince the users about the proposed anomoaly detection method,
but also helping the users to understand behavior about
the .
This process will help the users to make decision such as
adjusting some feature of sample according to propsed method to 
make the anomalier to normal.

% Unlike classic anomoaly detection method,
% debate and negotiation are often necessary for group users,
% and this calls for understanding of not only the pros but also
% the cons of the proposed recommendations.
% While existing explanation approaches focus on
% determining how good the recommendation is for the user,
% it is now desirable to know how bad the recommendation is for each group user.

\section{Evaluation of Anomaly Detection}

\subsection{Supervised Evaluation}

\subsection{Unsupervised Evaluation}

\section{Anomaly detection software packages}~\label{sec:tools}
Although many companies have implemented their own Anomaly detection systems 
to accommodate their specific business needs, 
there are still many free/ open source recommender system
software packages available.
In this section, 
we review some popular software packages for practitioners
to build their anomaly detection systems.

\subsection{ADTK}

Anomaly Detection Toolkit (ADTK) (\href{https://adtk.readthedocs.io}{https://adtk.readthedocs.io}) is a Python package for 
unsupervised / rule-based time series anomaly detection.

\subsection{PyOdds}

PyODDS (\href{http://pyodds.com/}{http://pyodds.com/}) is an end-to end Python system for
outlier detection with database support. 
PyODDS provides outlier detection algorithms, 
which support both static and time-series data.

\subsection{PyOD}

PyOD~\cite{zhao2019pyod} (\href{https://pypi.org/project/pyod/}{https://pypi.org/project/pyod/}) 
is a comprehensive and scalable Python toolkit for 
detecting outlying objects in multivariate data. 
It contains more than 20 detection algorithms, 
including emerging deep learning models and 
outlier ensembles.

\subsection{Anomaly Detection Toolbox}

Anomaly Detection Toolbox 
(\href{http://dsmi-lab-ntust.github.io/AnomalyDetectionToolbox/}{http://dsmi-lab-ntust.github.io/AnomalyDetectionToolbox/})
collectes a lot of popular outlier detection algorithms in Matlab.

\subsection{DeepADoTS}

DeepADoTS (\href{https://github.com/KDD-OpenSource/DeepADoTS}{https://github.com/KDD-OpenSource/DeepADoTS})
is a benchmarking pipeline for anomaly detection on
time series data for 
multiple state-of-the-art deep learning methods.

\subsection{RRCF}
RRCF~\cite{bartos_2019_rrcf} (\href{https://travis-ci.org/github/kLabUM/rrcf}{https://travis-ci.org/github/kLabUM/rrcf})



\section{Conclusions} \label{sec-conclusions}

\blindtext

\section*{Acknowledgment}

\lipsum[1]


The authors would like to thank \ldots

